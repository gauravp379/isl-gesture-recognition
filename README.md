# ğŸ¤Ÿ Indian Sign Language (ISL) to Text Conversion System

An AI-powered real-time gesture recognition system that interprets Indian Sign Language gestures and converts them into text and speech output. This system bridges the communication gap between the deaf/hearing-impaired community and non-sign language users.

## ğŸš€ Features

- **Real-time Gesture Recognition**: Live ISL gesture detection using webcam with MediaPipe hand tracking
- **Dual-Hand Support**: Recognizes both one-hand and two-hand gestures simultaneously
- **Comprehensive Alphabet & Numbers**: Supports ISL alphabets (A-Z) and digits (0-9)
- **Advanced Text-to-Speech**: Multiple TTS engines (pyttsx3, gTTS) with customizable voice settings
- **Intelligent NLP Processing**: Context-aware sentence formation with word completion and suggestions
- **Dynamic Gestures**: Support for motion-based gestures using LSTM networks (planned)
- **Interactive Web Interface**: Modern dashboard with live video feed and real-time feedback
- **Educational Games**: Alphabet and numbers learning games with progress tracking
- **Performance Monitoring**: Real-time accuracy, FPS, and system resource tracking
- **RESTful API**: Complete API endpoints for integration and custom applications
- **Multi-Platform Support**: Cross-platform compatibility (Windows, Linux, macOS)
- **Configurable System**: Extensive configuration options for different environments

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Web Browser   â”‚    â”‚   Flask Server   â”‚    â”‚   AI Models     â”‚
â”‚                 â”‚    â”‚                  â”‚    â”‚                 â”‚
â”‚ â€¢ HTML/CSS/JS   â”‚â—„â”€â”€â–ºâ”‚ â€¢ REST API       â”‚â—„â”€â”€â–ºâ”‚ â€¢ CNN Model     â”‚
â”‚ â€¢ Video Stream  â”‚    â”‚ â€¢ WebSocket      â”‚    â”‚ â€¢ LSTM Model    â”‚
â”‚ â€¢ Games         â”‚    â”‚ â€¢ Template Engineâ”‚    â”‚ â€¢ Hand Tracking â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                       â”‚
         â–¼                       â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ MediaPipe Hand  â”‚    â”‚  NLP Processor   â”‚    â”‚ Text-to-Speech  â”‚
â”‚ Tracking        â”‚    â”‚                  â”‚    â”‚                 â”‚
â”‚ â€¢ Landmark      â”‚â—„â”€â”€â–ºâ”‚ â€¢ Word Formation â”‚â—„â”€â”€â–ºâ”‚ â€¢ pyttsx3       â”‚
â”‚   Detection     â”‚    â”‚ â€¢ Sentence Build â”‚    â”‚ â€¢ gTTS          â”‚
â”‚ â€¢ Pose Analysis â”‚    â”‚ â€¢ Auto-complete  â”‚    â”‚ â€¢ Voice Config  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ Project Structure

```
isl-gesture-recognition/
â”œâ”€â”€ app.py                          # Main Flask application
â”œâ”€â”€ run.py                          # Application launcher script
â”œâ”€â”€ config.py                       # Configuration management
â”œâ”€â”€ requirements.txt                # Python dependencies
â”œâ”€â”€ README.md                       # Project documentation
â”œâ”€â”€ quick_test.py                   # Quick testing script
â”œâ”€â”€ test_gesture_recognition.py     # Unit tests
â”‚
â”œâ”€â”€ src/                            # Source code
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ models/                     # Machine learning models
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ gesture_recognizer.py   # Main recognition class
â”‚   â”‚   â””â”€â”€ train_cnn.py           # CNN model training
â”‚   â”œâ”€â”€ utils/                      # Utility modules
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ tts_handler.py         # Text-to-speech handler
â”‚   â”‚   â”œâ”€â”€ nlp_processor.py       # Natural language processing
â”‚   â”‚   â””â”€â”€ performance_monitor.py # Performance monitoring
â”‚   â””â”€â”€ preprocessing/              # Data preprocessing
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ preprocess_data.py     # Data preparation scripts
â”‚
â”œâ”€â”€ static/                         # Static web assets
â”‚   â”œâ”€â”€ css/
â”‚   â”‚   â”œâ”€â”€ style.css              # Main stylesheet
â”‚   â”‚   â””â”€â”€ style-game.css        # Game-specific styles
â”‚   â”œâ”€â”€ js/
â”‚   â”‚   â””â”€â”€ app.js                # Frontend JavaScript
â”‚   â”œâ”€â”€ images/                   # Static images
â”‚   â”œâ”€â”€ gestures/                 # Gesture demonstration images
â”‚   â””â”€â”€ uploads/                  # User upload directory
â”‚
â”œâ”€â”€ templates/                     # HTML templates
â”‚   â”œâ”€â”€ index.html                # Main dashboard
â”‚   â”œâ”€â”€ learn.html                # Learning interface
â”‚   â”œâ”€â”€ alphabet_game.html        # Alphabet learning game
â”‚   â”œâ”€â”€ numbers_game.html         # Numbers learning game
â”‚   â””â”€â”€ errors/                   # Error page templates
â”‚       â”œâ”€â”€ 404.html
â”‚       â””â”€â”€ 500.html
â”‚
â”œâ”€â”€ data/                         # Data management
â”‚   â”œâ”€â”€ raw/                      # Raw gesture data
â”‚   â”œâ”€â”€ processed/                # Preprocessed datasets
â”‚   â””â”€â”€ datasets/                 # Training datasets
â”‚
â”œâ”€â”€ models/                       # Trained models and artifacts
â”‚   â”œâ”€â”€ trained/                  # Production models
â”‚   â”‚   â”œâ”€â”€ cnn_model.h5         # Trained CNN model
â”‚   â”‚   â”œâ”€â”€ cnn_model_config.json # Model configuration
â”‚   â”‚   â””â”€â”€ *_labels.pkl         # Class labels
â”‚   â”œâ”€â”€ checkpoints/              # Training checkpoints
â”‚   â”œâ”€â”€ plots/                    # Training visualizations
â”‚   â””â”€â”€ evaluation_results.json   # Model evaluation metrics
â”‚
â”œâ”€â”€ logs/                         # Application logs
â”‚   â””â”€â”€ performance.log           # Performance monitoring logs
â”‚
â”œâ”€â”€ tests/                        # Test suites
â”œâ”€â”€ docs/                         # Documentation
â””â”€â”€ frontend/                     # Alternative frontend (legacy)
    â”œâ”€â”€ index.html
    â”œâ”€â”€ learn.html
    â”œâ”€â”€ alphabet-game.html
    â””â”€â”€ static/
```

## ğŸ”§ Installation

### Prerequisites

- **Python 3.7+**: Required for TensorFlow compatibility
- **Webcam**: For real-time gesture recognition
- **Git**: For cloning the repository

## ğŸ¯ Usage

### Basic Usage

1. **Start the Application**: Run `python run.py`
2. **Allow Camera Access**: Grant webcam permissions when prompted
3. **Position Hands**: Place hands clearly in front of camera (6-12 inches away)
4. **Make Gestures**: Perform ISL signs for letters A-Z or numbers 0-9
5. **View Results**: See real-time text conversion on screen
6. **Listen to Speech**: Click "Speak" button to hear the text

### Advanced Features

#### Real-time Gesture Recognition
- **Confidence Threshold**: Gestures below 60% confidence are ignored
- **Gesture Interval**: New gestures captured every 2 seconds
- **Multi-hand Support**: Recognizes gestures from both hands simultaneously

#### Text-to-Speech
- **Multiple Engines**: Automatic fallback between pyttsx3 and gTTS
- **Voice Customization**: Adjustable rate, volume, and voice selection
- **Async Processing**: Non-blocking speech synthesis

#### NLP Processing
- **Word Completion**: Automatic completion of partial words
- **Sentence Formation**: Intelligent sentence building from gesture sequences
- **Word Suggestions**: Context-aware word suggestions

#### Educational Games
- **Alphabet Game**: Interactive alphabet learning with scoring
- **Numbers Game**: Number recognition practice
- **Progress Tracking**: Learning progress visualization

### Keyboard Shortcuts

- **Space**: Speak current sentence
- **Enter**: Add gesture to sentence
- **Backspace**: Clear last gesture
- **Ctrl+C**: Clear entire sentence

## ğŸ“¡ API Documentation

### Core Endpoints

#### GET `/`
Main dashboard interface

#### GET `/video_feed`
MJPEG video stream with gesture recognition overlay

#### GET `/api/gesture_data`
Get current gesture recognition data
```json
{
  "current_sentence": "HELLO WORLD",
  "current_gesture": "H",
  "confidence": 0.95,
  "hand_count": 1,
  "fps": 28.5,
  "timestamp": 1640995200.0
}
```

#### POST `/api/speak`
Convert text to speech
```json
{
  "text": "Hello World",
  "success": true
}
```

#### POST `/api/translate`
Translate text to Hindi
```json
{
  "text": "Hello World",
  "translated": "à¤¨à¤®à¤¸à¥à¤¤à¥‡ à¤¦à¥à¤¨à¤¿à¤¯à¤¾"
}
```

#### POST `/api/update_sentence`
Update current sentence
```json
{
  "sentence": "New sentence text"
}
```

#### POST `/api/clear_sentence`
Clear current sentence

#### GET `/api/performance`
Get performance metrics
```json
{
  "fps": 28.5,
  "avg_fps": 27.8,
  "accuracy": 0.94,
  "latency": 45.2,
  "system": {
    "cpu_percent": 45.2,
    "memory_percent": 62.1
  }
}
```

#### GET `/api/model_info`
Get model information
```json
{
  "model_type": "CNN",
  "input_shape": [null, 63],
  "num_classes": 37,
  "class_labels": ["A", "B", "C", ...],
  "avg_prediction_time": 0.035
}
```

### Game Endpoints

#### GET `/learn`
Learning interface

#### GET `/alphabet-game`
Alphabet learning game

#### GET `/numbers-game`
Numbers learning game

### Health Check

#### GET `/health`
System health status
```json
{
  "status": "healthy",
  "gesture_recognizer": true,
  "camera_available": true,
  "tts_available": true,
  "timestamp": 1640995200.0
}
```

#### GET `/debug`
Detailed debug information

## ğŸ§  Model Training

### Training Requirements

- **Dataset**: ISL gesture images (A-Z, 0-9)
- **Hardware**: GPU recommended for faster training
- **Time**: 2-4 hours for full training

### Training Process

1. **Prepare Dataset**
   ```bash
   python src/preprocessing/preprocess_data.py
   ```

2. **Train CNN Model**
   ```python
   from src.models.train_cnn import train_cnn_model

   # Train the model
   model = train_cnn_model(
       data_dir='data/processed',
       epochs=50,
       batch_size=32,
       validation_split=0.2
   )
   ```

3. **Evaluate Model**
   ```python
   from src.models.gesture_recognizer import GestureRecognizer

   # Load and evaluate
   recognizer = GestureRecognizer()
   metrics = recognizer.evaluate_model()
   ```

4. **Save Model**
   ```python
   recognizer.save_model('models/trained/cnn_model.h5')
   ```

### Model Configuration

```python
# Model hyperparameters
config = {
    'input_shape': (63,),  # 21 landmarks * 3 coordinates
    'num_classes': 37,     # A-Z + 0-9 + special symbols
    'batch_size': 32,
    'epochs': 50,
    'learning_rate': 0.001,
    'dropout_rate': 0.3
}
```

### Configuration Classes

- **DevelopmentConfig**: Debug mode, detailed logging
- **ProductionConfig**: Optimized for production deployment
- **TestingConfig**: Disabled camera, mock data

### Performance Optimization

1. **Reduce Camera Resolution**
2. **Lower Confidence Threshold**
3. **Disable Performance Logging**
4. **Use GPU for Inference**
5. **Optimize Model Size**

## ğŸ“Š Performance Metrics

### Current Benchmarks

- **Accuracy**: ~95% on test dataset (A-Z, 0-9)
- **FPS**: 25-30 frames per second
- **Latency**: 35-50ms per prediction
- **Memory Usage**: ~500MB RAM
- **CPU Usage**: 40-60% during recognition

## ğŸ› ï¸ Technical Stack

### Backend
- **Framework**: Flask 2.3.3
- **Language**: Python 3.7+
- **ASGI Server**: Werkzeug (development), Gunicorn (production)

### Computer Vision & AI
- **Computer Vision**: OpenCV 4.8.1
- **Hand Tracking**: MediaPipe 0.10.7
- **Deep Learning**: TensorFlow 2.13.0, Keras 2.13.1
- **Data Processing**: NumPy 1.24.3, Pandas 2.0.3

### Natural Language Processing
- **Core NLP**: NLTK 3.8.1, spaCy 3.6.1
- **Text Processing**: scikit-learn 1.3.0

### Text-to-Speech
- **Primary Engine**: pyttsx3 2.90
- **Fallback Engine**: gTTS 2.3.2
- **Audio Processing**: pygame (for gTTS)

### Frontend
- **HTML5**: Semantic markup
- **CSS3**: Responsive design, animations
- **JavaScript**: ES6+ features, async/await
- **Real-time Updates**: Server-Sent Events

### Development & Testing
- **Testing**: pytest 7.4.2, pytest-flask 1.2.0
- **Code Quality**: black 23.7.0, flake8 6.0.0
- **Type Checking**: mypy (optional)
- **Performance**: psutil 5.9.5

### Deployment
- **Containerization**: Docker, docker-compose
- **Process Management**: systemd, supervisor
- **Reverse Proxy**: Nginx
- **SSL/TLS**: Let's Encrypt

## ğŸ”® Future Enhancements

### Short Term (3-6 months)
- [ ] Expand vocabulary to 500+ common ISL words
- [ ] Add dynamic gesture recognition (motion-based)
- [ ] Implement user authentication and profiles
- [ ] Add gesture recording and custom model training
- [ ] Mobile application (React Native)

### Medium Term (6-12 months)
- [ ] Multi-language support (Hindi, regional languages)
- [ ] Real-time collaboration features
- [ ] Cloud deployment with auto-scaling
- [ ] Integration with video conferencing platforms
- [ ] Advanced NLP with context understanding

### Long Term (1-2 years)
- [ ] Full ISL grammar and syntax support
- [ ] Emotion recognition from facial expressions
- [ ] Sign language translation between different regions
- [ ] Educational platform with curriculum
- [ ] Professional interpreter assistance system

### Technical Improvements
- [ ] Model optimization (quantization, pruning)
- [ ] Edge computing deployment
- [ ] Real-time model updates
- [ ] Multi-modal input (voice + gesture)
- [ ] Advanced computer vision techniques  

### Contribution Guidelines

- **Code Style**: Follow PEP 8 and use Black formatter
- **Testing**: Maintain >80% test coverage
- **Documentation**: Update README and docstrings
- **Commits**: Use conventional commit messages
- **Issues**: Use issue templates for bug reports and features

### Areas for Contribution

- **Model Improvement**: Better accuracy, new gesture types
- **UI/UX Enhancement**: Better user interface and experience
- **Performance Optimization**: Faster inference, lower latency
- **Documentation**: Tutorials, API docs, video guides
- **Testing**: More comprehensive test suites
- **Internationalization**: Support for other sign languages

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

### Permissions
- âœ… Commercial use
- âœ… Modification
- âœ… Distribution
- âœ… Private use

### Limitations
- âŒ Liability
- âŒ Warranty

### Conditions
- ğŸ“ License and copyright notice

## ğŸ™ Acknowledgments

### Core Technologies
- **MediaPipe Team**: Revolutionary hand tracking technology
- **TensorFlow Team**: Powerful deep learning framework
- **OpenCV Community**: Computer vision excellence
- **Flask Team**: Lightweight web framework

### Research & Data
- **ISL Research Community**: Gesture datasets and research papers
- **Deaf Community**: Valuable feedback and testing
- **Educational Institutions**: Research collaborations

---

**Made with â¤ï¸ for the deaf and hearing-impaired community**
