# ğŸ¤Ÿ Indian Sign Language (ISL) to Text Conversion System

An AI-powered real-time gesture recognition system that interprets Indian Sign Language gestures and converts them into text and speech output. This system bridges the communication gap between the deaf/hearing-impaired community and non-sign language users.

## ğŸš€ Features

- **Real-time Gesture Recognition**: Live ISL gesture detection using webcam
- **Dual-Hand Support**: Recognizes both one-hand and two-hand gestures
- **Alphabet & Numbers**: Supports ISL alphabets (A-Z) and digits (0-9)
- **Text-to-Speech**: Converts recognized text to audible speech
- **Dynamic Gestures**: Handles motion-based gestures using LSTM networks
- **Web Interface**: Interactive dashboard with live video feed
- **Performance Monitoring**: Real-time accuracy and FPS tracking
- **NLP Integration**: Context-aware sentence formation

## ğŸ—ï¸ Architecture

```
User Gesture â†’ Camera Capture â†’ MediaPipe Hand Tracking â†’ 
Feature Extraction â†’ CNN/LSTM Model â†’ NLP Processing â†’ 
Text Output â†’ Speech Synthesis
```

## ğŸ“ Project Structure

```
isl-gesture-recognition/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ models/           # Deep learning models (CNN, LSTM)
â”‚   â”œâ”€â”€ utils/            # Utility functions
â”‚   â””â”€â”€ preprocessing/    # Data preprocessing modules
â”œâ”€â”€ static/
â”‚   â”œâ”€â”€ css/             # Stylesheets
â”‚   â”œâ”€â”€ js/              # JavaScript files
â”‚   â””â”€â”€ images/          # Static images
â”œâ”€â”€ templates/           # HTML templates
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/            # Raw gesture data
â”‚   â”œâ”€â”€ processed/      # Preprocessed data
â”‚   â””â”€â”€ datasets/       # Training datasets
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ trained/        # Trained model files
â”‚   â””â”€â”€ checkpoints/    # Model checkpoints
â”œâ”€â”€ tests/              # Unit tests
â”œâ”€â”€ docs/               # Documentation
â”œâ”€â”€ app.py              # Main Flask application
â”œâ”€â”€ requirements.txt    # Dependencies
â””â”€â”€ README.md          # Project documentation
```

## ğŸ”§ Installation

1. **Clone the repository**
   ```bash
   git clone <repository-url>
   cd isl-gesture-recognition
   ```

2. **Create virtual environment**
   ```bash
   python -m venv venv
   # Windows
   venv\Scripts\activate
   # Linux/Mac
   source venv/bin/activate
   ```

3. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

4. **Run the application**
   ```bash
   python app.py
   ```

5. **Open browser and navigate to**
   ```
   http://localhost:5000
   ```

## ğŸ¯ Usage

1. **Start the Application**: Run `python app.py`
2. **Allow Camera Access**: Grant webcam permissions when prompted
3. **Position Hands**: Place hands clearly in front of camera
4. **Make Gestures**: Perform ISL signs for letters A-Z or numbers 0-9
5. **View Results**: See real-time text conversion on screen
6. **Listen to Speech**: Click "Speak" to hear the text

## ğŸ§  Model Architecture

### Static Gesture Recognition
- **CNN Model**: Convolutional layers for feature extraction
- **Input**: Hand landmark coordinates (21 points per hand)
- **Output**: Probability distribution over 36 classes (A-Z, 0-9)

### Dynamic Gesture Recognition
- **CNN + LSTM**: Temporal sequence modeling
- **Input**: Sequential frames of hand movements
- **Output**: Motion-based gesture classification

## ğŸ“Š Performance Metrics

- **Accuracy**: ~95% on test dataset
- **FPS**: 30+ frames per second
- **Latency**: <100ms gesture-to-text conversion
- **Supported Gestures**: 36 (26 letters + 10 digits)

## ğŸ› ï¸ Technical Stack

- **Backend**: Flask (Python)
- **Computer Vision**: OpenCV, MediaPipe
- **Deep Learning**: TensorFlow/Keras
- **Frontend**: HTML5, CSS3, JavaScript
- **Text-to-Speech**: pyttsx3, gTTS
- **NLP**: NLTK, spaCy

## ğŸ”® Future Enhancements

- [ ] Expand to full ISL vocabulary (words & sentences)
- [ ] Multi-language support (Hindi, regional languages)
- [ ] Mobile app development
- [ ] Cloud deployment with API endpoints
- [ ] Real-time collaboration features
- [ ] Educational module for ISL learning

## ğŸ¤ Contributing

1. Fork the repository
2. Create feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit changes (`git commit -m 'Add AmazingFeature'`)
4. Push to branch (`git push origin feature/AmazingFeature`)
5. Open Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- MediaPipe team for hand tracking technology
- TensorFlow team for deep learning framework
- ISL research community for gesture datasets
- Open source contributors

## ğŸ“ Contact

For questions, suggestions, or collaboration opportunities, please reach out through the project issues or contact the development team.

---

**Made with â¤ï¸ for the deaf and hearing-impaired community**
